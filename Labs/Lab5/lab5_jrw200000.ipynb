{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYSM 6302 - Lab 5\n",
    "Jonas Wagner - jrw200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from numpy import zeros, dot, array\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preq processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "if not path.exists('raw_twitter.json'):\n",
    "    print('need to extract .json.zip file')    \n",
    "if not path.exists('small_raw_twitter.json'):\n",
    "    print('small version of raw_twitter does\\'nt exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.13: Modularity\n",
    "\n",
    "The first function below calculates modularity for *directed* networks and also returns the maximum modularity value $Q_{\\text{max}}$ (NetworkX's modularity function does not report the $Q_{\\text{max}}$ value). The second function calculates scalar assortativity (NetworkX's assortativity functions differ from our book definition). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(G,c):\n",
    "    d = dict()\n",
    "    for k,v in enumerate(c):\n",
    "        for n in v:\n",
    "            d[n] = k\n",
    "    L = 0\n",
    "    for u,v,data in G.edges.data():\n",
    "        L += data['weight']\n",
    "    Q, Qmax = 0,1\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if d[u] == d[v]:\n",
    "                Auv = 0\n",
    "                if G.has_edge(v,u):\n",
    "                    Auv = G[v][u]['weight']\n",
    "                Q += ( Auv - G.in_degree(u,weight='weight')*G.out_degree(v,weight='weight')/L )/L\n",
    "                Qmax -= ( G.in_degree(u,weight='weight')*G.out_degree(v,weight='weight')/L )/L\n",
    "    return Q, Qmax\n",
    "\n",
    "def scalar_assortativity(G,d):\n",
    "    x = zeros(G.number_of_nodes())\n",
    "    for i,n in enumerate(G.nodes()):\n",
    "        x[i] = d[n]\n",
    "\n",
    "    A = array(nx.adjacency_matrix(G).todense().T)\n",
    "    M = 2*A.sum().sum()\n",
    "    ki = A.sum(axis=1) #row sum is in-degree\n",
    "    ko = A.sum(axis=0) #column sum is out-degree\n",
    "    mu = ( dot(ki,x)+dot(ko,x) )/M\n",
    "\n",
    "    R, Rmax = 0, 0\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        for j in range(G.number_of_nodes()):\n",
    "             R += ( A[i,j]*(x[i]-mu)*(x[j]-mu) )/M\n",
    "             Rmax += ( A[i,j]*(x[i]-mu)**2 )/M\n",
    "\n",
    "    return R, Rmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIFA exports by geographic region is assortatively mixed: 0.1200/0.5505\n",
      "FIFA exports by importers/exporters is disassortatively mixed: -0.0185/0.5748\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_weighted_edgelist('fifa1998.edgelist',create_using=nx.DiGraph)\n",
    "\n",
    "c = {\n",
    "    'group1': {'Argentina','Brazil','Chile','Mexico','Colombia','Jamaica','Paraguay'},\n",
    "    'group2': {'Japan','SouthKorea'},\n",
    "    'group3': {'UnitedStates'},\n",
    "    'group4': {'Nigeria','Morocco','SouthAfrica','Cameroon','Tunisia','Iran','Turkey'},\n",
    "    'group5': {'Scotland','Belgium','Austria','Germany','Denmark','Spain','France','GreatBritain','Greece','Netherlands','Norway','Portugal','Italy','Yugoslavia','Romania','Bulgaria','Croatia','Switzerland'}\n",
    "    }\n",
    "Q, Qmax = modularity(G,c.values())\n",
    "print('FIFA exports by geographic region is assortatively mixed: %1.4f/%1.4f' % (Q,Qmax))\n",
    "\n",
    "c = {\n",
    "    'exporters': {'Argentina','Brazil','Chile','Colombia','Mexico','Scotland','Belgium','Austria','Denmark','France','Greece','Netherlands','Portugal','Yugoslavia','Croatia','Jamaica','Cameroon','Nigeria','Morocco','Tunisia'},\n",
    "    'importers': {'Paraguay','SouthKorea','UnitedStates','SouthAfrica','Iran','Turkey','Germany','Spain','GreatBritain','Norway','Italy','Romania','Bulgaria','Switzerland','Japan'}\n",
    "    }\n",
    "Q, Qmax = modularity(G,c.values())\n",
    "print('FIFA exports by importers/exporters is disassortatively mixed: %1.4f/%1.4f' % (Q,Qmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explination of Modularity Results\n",
    "FIFA exports by region demonstrates more assortive mixing then that of the exporters vs importers assortativity. Although I don't know much about FIFA (I'm definetly an Americain Football guy), I expect their to be more connections between countries in the same regions then with teams on different parts of the world, so this would make sence for it to be more assortative. On the other hand, the classes of importers and exporters doesn't seem to have any particular reason for interconnection between thoose in the same group (considering the definition of an import/export) so disassortative mixing is the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.13: Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assortativity by GDP: -0.0005\n",
      "Assortativity by life expectancy: 0.1281\n",
      "Assortativity by tariff: 0.1191\n"
     ]
    }
   ],
   "source": [
    "gdp = pickle.load(open('gdp.pkl','rb'))\n",
    "life_expectancy = pickle.load(open('life_expectancy.pkl','rb'))\n",
    "tariff = pickle.load(open('tariff.pkl','rb'))\n",
    "\n",
    "G = nx.read_weighted_edgelist('world_trade_2014.edgelist',create_using=nx.DiGraph)\n",
    "\n",
    "R, Rmax = scalar_assortativity(G,gdp)\n",
    "print('Assortativity by GDP: %1.4f' % (R/Rmax))\n",
    "R, Rmax =  scalar_assortativity(G,life_expectancy)\n",
    "print('Assortativity by life expectancy: %1.4f' % (R/Rmax))\n",
    "R, Rmax =  scalar_assortativity(G,tariff)\n",
    "print('Assortativity by tariff: %1.4f' % (R/Rmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explination of Assortativity Results\n",
    "The assortativity of trade based on GDP is near zero, indicating neither a assorative nor disortative mixing between contries of different GDP values. My guess as to why this is is becouse countries of higher GDP are not likely to only trade with other high GDP countries (it just wouldn't be smart) and similarily, small GDP countries would likely not trade any more with others with low GDP then thoose with Higher GDP.\n",
    "\n",
    "There is a higher assortativity between life expectancy (the same as covariance/correlation) and the amount of trade between nations. This is possibly becouse nations of higher life expectancy would trade more for luxory goods or technology, while nations with lower life expectancy are not going to trade as much.\n",
    "\n",
    "There also appears to be a correlation between average tarrif rates and the amount of trade between countries. This could posibly be due to the higher likelyhood of tarrifs to be placed on goods at a comparrable level to thoose of the nations you trade with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algebraic Manipulation of Covariance\n",
    "(*This could be done on paper... but latex is just better... I also may or may not have already done this for another class*)\n",
    "\n",
    "Let, $$\\mu = \\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l$$\n",
    "The Correlation is defined as:\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}\n",
    "A_{ij} (x_i - \\mu) (x_j - \\mu)$$\n",
    "This can then be expanded into\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}\n",
    "A_{ij} (x_i x_j - x_i \\mu - x_j \\mu + \\mu^2)$$\n",
    "The definition of the $\\mu$ can then be substituted in\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}\n",
    "A_{ij} (x_i x_j \n",
    "- (x_i + x_j) (\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l) \n",
    "+ (\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l)^2)$$\n",
    "This can then be expanded again into\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j)\\\\\n",
    "- \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij}\n",
    "(x_i + x_j) (\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l))\\\\\n",
    "+ \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}\n",
    "(\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l)^2)\n",
    "$$\n",
    "And expanded again\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j)\\\\\n",
    "- \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}(x_i)\n",
    "(\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l))\\\\\n",
    "- \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}(x_j)\n",
    "(\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l))\\\\\n",
    "+ \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}\n",
    "(\\frac{1}{2m})^2 (k_1^2 x_1^2 + k_1 k_2 x_1 x_2 + ... + k_n^2 x_n^2))\n",
    "$$\n",
    "and again (noting that the appropriete A_ij terms are encorporated into the expanded sum)\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j)\\\\\n",
    "- \\frac{n^2}{(2m)^3}  (k_1^2 x_1^2 + k_1 k_2 x_1 x_2 + ... + k_n^2 x_n^2)\\\\\n",
    "- \\frac{n^2}{(2m)^3} (k_1^2 x_1^2 + k_1 k_2 x_1 x_2 + ... + k_n^2 x_n^2)\\\\\n",
    "+ (\\frac{1}{2m})^3 (k_1^2 x_1^2 + k_1 k_2 x_1 x_2 + ... + k_n^2 x_n^2))\n",
    "$$\n",
    "then eliminating the terms\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j)\\\\\n",
    "- (\\frac{n}{2m})^3  (k_1^2 x_1^2 + k_1 k_2 x_1 x_2 + ... + k_n^2 x_n^2)\n",
    "$$\n",
    "then compacting (again remembering that the k_i k_j will be zero for the cases the A_ij is zero)\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j)\\\\\n",
    "- (\\frac{n}{2m})^3 (\\sum_{l = 1}^{n} k_l x_l)^2\n",
    "$$\n",
    "and again\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j)\\\\\n",
    "- \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} (\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l)^2\n",
    "$$\n",
    "and again\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j) - (\\frac{1}{2m} \\sum_{l = 1}^{n} k_l x_l)^2\n",
    "$$\n",
    "and again\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} x_i x_j) - \\mu^2$$\n",
    "which is equivelent to (clearly from the definition of \\mu^2 being the full expanded sums again)\n",
    "$$R = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n}(A_{ij} - \\frac{k_i k_j}{2m}) x_i x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections 11.2-11.11: Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Need to do the KL algorithm example drawing***\n",
    "Tips:\n",
    "- Go in reverse from perfect to scrambled and make the cut set larger... then how do you make it smaller again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modularity Matrix Summation Proof\n",
    "\n",
    "Let,\n",
    "$$B_{ij} = A_{ij} - \\frac{k_i k_j}{2m}$$\n",
    "\n",
    "When summing over all collums for a single row, the sum can be shown to be zero as follows\n",
    "$$\\sum_{j=1}^n B_{ij} \n",
    "= \\sum_{j=1}^n A_{ij} - \\frac{k_i k_j}{2m}\\\\\n",
    "= \\sum_math\n",
    "$$\n",
    "\n",
    "note... sum all row or collumn = degree of node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection in Practice\n",
    "Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for running sections of community detection\n",
    "small = 'small' # 'small_' if small... '' if full\n",
    "raw_tweets_filename = small + 'raw_twitter'\n",
    "hashtag_sets_filename = small + 'hashtag_sets'\n",
    "edgelist_filename = small + 'edgelist'\n",
    "htag_comms_filename = small + 'htag_communities'\n",
    "#use small_raw_twitter and small_hashtag_sets for speed\n",
    "\n",
    "# Run sections\n",
    "json_2_raw_tweets = not path.exists(raw_tweets_filename + '.txt')\n",
    "raw_tweets_2_hashtag_sets = not path.exists(hashtag_sets_filename + '.txt')\n",
    "build_network = not path.exists(edgelist_filename + '.edgelist')\n",
    "find_communities_original = False # Don't run... not needed\n",
    "find_communities_w10_c10 = not path.exists(htag_comms_filename + '_w10_c10' + '.txt')\n",
    "# find_communities_w10_c10 = not path.exists(htag_comms_filename + '_w10_c10' + '.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Network Conceptual Questions\n",
    "The idea of linking hashtags together by when they occur together in a tweet could describe the similarity between them as they are used together on the same message. This would potray a similar feeling/tone or related topics of interest.\n",
    "An example of this would be the use of #fml and #disapointed. In tweets when they are used together they likely are used to describe the feeling dispair or sadness that refer to the contents of the tweet.\n",
    "An example that is not as useful would be when two hashtags are used in a tweet as polar opesites, i.e. #Good vs #Bad, or #Fire vs #Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for dealing with tweets\n",
    "*** It is getting anoying to have to do this more complicated in a jupyter notebook... I'm looking foward to doing it all with scripts for my project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON2List(filename):\n",
    "    \"\"\"\n",
    "    this reads a raw json, selects only the english tweets\n",
    "    and saves text to a list of strings\n",
    "    \"\"\"\n",
    "    fp = open(filename + '.json', 'r', encoding='utf-8')\n",
    "    tweets = []\n",
    "    for line in fp:\n",
    "        if len(line) > 2:\n",
    "            line_data = json.loads(line)\n",
    "            if line_data['lang'] == 'en': # only english\n",
    "                tweets.append(line_data['text'])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTxt2List(filename, deliminator = '\\n uniqueDeliminator \\n'):\n",
    "    \"\"\"\n",
    "    reads from outputed txt list into a python list\n",
    "    \"\"\"\n",
    "    print('not coded...')\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeList2txt(tweets, filename, deliminator = '\\n uniqueDeliminator \\n'):\n",
    "    \"\"\"\n",
    "    this writes a list of tweets to a txt file\n",
    "    \"\"\"\n",
    "    with open(filename + '.txt', 'w', encoding = 'utf-8') as filehandle:\n",
    "        for tweet in tweets:\n",
    "            filehandle.write(tweet + deliminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetTextAnalysis(tweet):\n",
    "    \"\"\"\n",
    "    takes tweet text (as a string) and processes it...\n",
    "    1) strip\n",
    "    2) lowercase\n",
    "    3) hashtag search\n",
    "    4) return list of hashtags (withough #)\n",
    "    \"\"\"\n",
    "    if '#' in tweet:\n",
    "        tweet = tweet.strip()\n",
    "        tweet = tweet.lower()\n",
    "        regex = \"#(\\w+)\"\n",
    "        hashtags = sorted(re.findall(regex, tweet))\n",
    "        if len(hashtags) == 0:\n",
    "            hashtags = -1\n",
    "    else:\n",
    "         hashtags = -1   \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findHashtagSets(tweets):\n",
    "    \"\"\"\n",
    "    This interprits the list of tweets (english and post striping of json stuff)\n",
    "    and finds the co-occuring hashtags\n",
    "    \"\"\"\n",
    "    hashtagSets = [str]\n",
    "    for tweet in tweets:\n",
    "        hashtags = tweetTextAnalysis(tweet)\n",
    "        if type(hashtags) == list:\n",
    "            if len(hashtags) > 1:\n",
    "                hashtagSets.append(hashtags)\n",
    "                print(hashtags)\n",
    "    return hashtagSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for building and analyzing networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@elissakh the one and only Queen ❤️💝 #Toronto https://t.co/PZTZW5GCQk ['toronto']\n"
     ]
    }
   ],
   "source": [
    "tweet = tweets[108]\n",
    "hashtag = tweetTextAnalysis(tweet)\n",
    "print(tweet, hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Identifying co-occuring hashtags in data\n",
    "\n",
    "1. Use JSON to output tweet data directly into a text file: raw_tweets.txt\n",
    "2. Use raw_tweets.txt to produce a list of space-deliminated list of hashtages in tweets: hashtag_sets.txt\n",
    "\n",
    "#### Empty line explination\n",
    "All of the empty lines (that I didn't purposly add) within the hashtag_sets.txt file indicate that no hashtags were included in the respective tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON 2 Raw Tweets\n",
    "if json_2_raw_tweets:\n",
    "    tweets = readJSON2List(raw_tweets_filename)\n",
    "    writeList2txt(tweets, raw_tweets_filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "#     tweets = readTxt2List(raw_tweets_filename) #isn't coded yet\n",
    "    tweets = readJSON2List(raw_tweets_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나인뮤지스', '더쇼']\n",
      "['aries', 'ascendant', 'leo', 'mediumcoeli']\n",
      "['gmos', 'neonicotinoid', 'pesticides']\n",
      "['etsymntt', 'pott']\n",
      "['aaronrodgers', 'sports']\n",
      "['filipina', 'philippines', 'pinay']\n",
      "['camaradoalmirante', 'rio2016']\n",
      "['lingerie', 'pantyhose', 'stockings']\n",
      "['mtvstars', 'mtvstars']\n",
      "['gameinsight', 'ipad', 'ipadgames']\n",
      "['40', 'pizero']\n",
      "['liesonsoundcloud', 'welcometweet']\n",
      "['karachistandswit', 'mqm']\n",
      "['job', 'jobs', 'lpn', 'tucson']\n",
      "['csgo', 'csgofast', 'csgogiveaway', 'csgoskins']\n",
      "['tommorowwhenigetthevampsalbumiwill', 'vampsalbumouttomorrow']\n",
      "['blackfriday', 'kohlssweepstakes']\n",
      "['cleveland', 'detroit', 'takeover']\n",
      "['firefriday', 'teamoptimist']\n",
      "['biafra', 'freebiafra', 'freennamdikanu']\n",
      "['gratuitousfaggotry', 'yoloswagpovverbottomsforjesus']\n",
      "['jobs', 'maintenance', 'manager', 'rugby']\n",
      "['infection', 'undead', 'zombie']\n",
      "['heath', 'sugar']\n",
      "['forex', 'trading']\n",
      "['mtvstars', 'mtvstars']\n",
      "['carvsdal', 'happythanksgiving']\n",
      "['naruto', 'sasusaku']\n",
      "['cowboys', 'nflthanksgiving', 'thanksgiving']\n",
      "['money', 'work']\n",
      "['funds', 'investment', 'investment', 'risk', 'southafrica']\n",
      "['target', 'taylorswift', 'taylorswiftdress', 'taylorswiftfans', 'taylorswifttee']\n",
      "['blackfriday', 'champagne', 'monopolemoments', 'win']\n",
      "['liesonsoundcloud', 'welcometweet']\n",
      "['android', 'androidgames', 'gameinsight']\n",
      "['action', 'dreams', 'jimrohn', 'passion']\n",
      "['climatechange', 'cop21']\n",
      "['football', 'jets', 'nfl', 'tbt', 'thanksgiving']\n",
      "['brony', 'christmas', 'mlp']\n",
      "['free', 'kindle']\n",
      "['iclaimthisone', 'politicalrefugee', 'starsareborn', 'thefolksman']\n",
      "['mtvstars', 'videomtv2015']\n",
      "['blackfriday', 'rtept']\n",
      "['mediaawards2015', 'w4r']\n",
      "['5sos', 'bands', 'selfie', 'taco']\n",
      "['ascendant', 'gemini', 'mediumcoeli', 'virgo']\n",
      "['baileyboys', 'gulfbreezemom', 'happylife', 'portofino', 'turkeyday']\n",
      "['card', 'gaia', 'japan', 'pokemon']\n",
      "['listenlive', 'nowplaying']\n",
      "['blacklisted', 'sip']\n",
      "['mtvstars', 'mtvstars']\n",
      "['imaceleb', 'ladyc']\n",
      "['acechan', 'alleyesonme', 'greenmoneymafia']\n",
      "['art', 'artist', 'balance', 'craft', 'mp3', 'pottery']\n",
      "['etsy', 'ralphwaldoemerson']\n",
      "['tomorrowwhenigetthevampsalbumiwill', 'vampsalbumouttomorrow']\n",
      "['2201', '3221']\n",
      "['hemlockgrove', 'thefinalchapter']\n",
      "['stevenuniverse', 'thanksgiving']\n",
      "['detroit', 'happythanksgiving']\n",
      "['love', 'thanksgiving15']\n",
      "['bullybirds', 'clampclamp', 'dirtybirds', 'nokeysjust']\n",
      "['southafrica', 'tourism', 'visitstelle']\n",
      "['1', '1', '2', '2', '2', '2', '2', '3', '3', 'happychanyeolday']\n",
      "['hiimfitz', 'wildrosekennels']\n",
      "['cleveland', 'job', 'jobs', 'manager', 'solution']\n",
      "['careerarc', 'customerservice', 'hiring', 'job', 'jobs', 'nashua']\n",
      "['np', 'soundcloud']\n",
      "['diet', 'nyc']\n",
      "['mtvstars', 'mtvvideo2015']\n",
      "['allah', 'instagram', 'islam', 'muslim', 'photo', 'photoofth', 'prophetmuhammadpbuh', 'quran']\n",
      "['digdis', 'nowplaying']\n",
      "['arizona', 'freshair', 'homefortheholidays', 'nature', 'nofilter', 'pine']\n",
      "['3221', '3320']\n",
      "['doingood', 'sponsored']\n",
      "['justiceforwhatafeeling', 'mtvstars']\n",
      "['giditraffic', 'smoothride']\n",
      "['davematthewsband', 'dmb']\n",
      "['mtvstars', 'mtvstars']\n",
      "['apple', 'motion', 'starwars']\n",
      "['heavymetal', 'music', 'np']\n",
      "['cowboysnation', 'dallascowboys']\n",
      "['american', 'backsweep', 'ruthless']\n",
      "['hiring', 'hospitality', 'job', 'leesburg']\n",
      "['decriminalization', 'legalization', 'marijuana', 'propaganda']\n",
      "['homes', 'tampa']\n",
      "['25novembre', 'gouste', 'noalaviolenciadegenero', 'vpn']\n",
      "['3dprinting', 'additivemanufacturing', 'formnext15']\n",
      "['100faces', 'sanayairani']\n",
      "['08', 'starwars']\n",
      "['confident', 'thankful']\n",
      "['mtvstars', 'throwback']\n",
      "['mtvstars', 'mtvstars']\n",
      "['mijoo', 'sujeong', '러블리즈']\n",
      "['business', 'entrepreneur']\n",
      "['blackfriday', 'bogo', 'happyholidays', 'happylife', 'happythankgiving', 'sale', 'shimmy']\n",
      "['buenosaires', 'businessmgmt', 'hiring', 'job']\n",
      "['autographhotels', 'mrpoints']\n",
      "['mtvstars', 'videomtv2015']\n",
      "['boxing', 'champ']\n",
      "['galaxys4', 'galaxys4leather', 'galaxys4portel', 'galaxys4sleeve']\n",
      "['7544', '8392']\n",
      "['almeria', 'asturias', 'barcelona', 'galicia', 'granada', 'huelva', 'madrid', 'mallorca', 'marbella']\n",
      "['carcrisis', 'happythanksgiving']\n",
      "['brandyli', 'spotify', 'touchurbutt', 'touchyourbutt']\n",
      "['a11', 'd29', 'thangamagan']\n",
      "['activism', 'environment']\n",
      "['chivsgb', 'th4nksgiving']\n",
      "['carmilla', 'hollstein', 'negovanman']\n",
      "['d4nky', 'gamertag', 'xboxlive']\n",
      "['contest', 'contestalert', 'nwil', 'whycantisleep']\n",
      "['wordbrain', 'words']\n",
      "['leicester', 'musicforthesoul', 'sufi']\n",
      "['blackfriday', 'kohlssweepstakes']\n",
      "['gameinsight', 'iphone', 'iphonegames']\n",
      "['cool', 'thecherie']\n",
      "['gameinsight', 'iphone', 'iphonegames']\n",
      "['pjnet', 'supportcoachkennedy']\n",
      "['mtvstars', 'mtvstars']\n",
      "['arlington', 'cleaner', 'general', 'job', 'jobs']\n",
      "['amberrose2125', 'f4f', 'followme', 'mgwv', 'rt2gain', 'swissnews2015', 'teamfree']\n",
      "['anonymous', 'cleveland']\n",
      "['ad', 'coupon', 'mezzettamemories', 'save']\n",
      "['eagles', 'gobigblue', 'keepchip']\n",
      "['jobs', 'platform', 'sanfrancisco', 'solution']\n",
      "['dublinhour', 'laundrie', 'powerofthehours']\n",
      "['mtvstars', 'mtvstars']\n",
      "['applenews', 'ipad', 'iphone']\n",
      "['careerarc', 'countryclubhills', 'hiring', 'it', 'job', 'jobs']\n",
      "['tomorrowwhenigetthevampsalbumiwill', 'vampsalbumouttomorrow']\n",
      "['f4f', 'followme', 'kentheloondude', 'mgwv', 'rt2gain', 'swissnews2015', 'teamfree']\n",
      "['mtvstars', 'mtvstars']\n",
      "['allerton', 'bd8', 'bradford', 'lost', 'stolen']\n",
      "['cancerlosthisbattle', 'leahstrong']\n",
      "['thanksgiving', 'thanksgivingwithdominicans']\n",
      "['activism', 'environment']\n",
      "['forex', 'traiding']\n",
      "['blackfriday', 'competition', 'discount', 'entertowin', 'giveaway']\n",
      "['christmas', 'truthinthetinsel']\n",
      "['animalphotogr', 'bluedarkart_photography', 'cute', 'cutie', 'green', 'leaf', 'lizard', 'zandoli', 'zandoli_lizard']\n",
      "['100faces', 'kimhyunjoong']\n",
      "['porn', 'taylorswift']\n",
      "['artist', 'designers', 'madrid']\n",
      "['beatthecowboys', 'panther', 'panthernation', 'thankful']\n",
      "['assetstore', 'unity3d']\n",
      "['qbochat', 'thankful']\n",
      "['gocowboys', 'thanksgiving']\n",
      "['blessed', 'ds18team', 'enjoying', 'happylife', 'thanksgiving', 'welikeitloud']\n",
      "['mtvstars', 'mtvstars']\n",
      "['handicapper', 'vicmontesports', 'vii']\n",
      "['f4f', 'followme', 'mahmoud93326479', 'mgwv', 'rt2gain', 'swissnews2015', 'teamfree']\n",
      "['bigdata', 'iot']\n",
      "['cem', 'custserv', 'cx']\n",
      "['cavuto', 'foxnews']\n",
      "['frozen', 'olaf']\n",
      "['activism', 'environment']\n",
      "['refugees', 'tromsø']\n",
      "['germanshepherd', 'purebred']\n",
      "['1388', '81655']\n",
      "['favre', 'gopackgo', 'th4nksgiving']\n",
      "['foodtrucksvic', 'victoria']\n",
      "['blackdayturkey', 'candundar']\n",
      "['eruption', 'minions']\n",
      "['fincher', 'zodiac']\n",
      "['imaceleb', 'imaceleb', 'ladyc']\n",
      "['foodporn', 'thanksgiving']\n",
      "['guysthatlift', 'guyswithmuscle', 'ontariofit', 'ontariomuscle']\n",
      "['analytics', 'bigdata', 'data', 'experience', 'iot', 'wisdom']\n",
      "['cruise2016', 'royalcaribbean']\n",
      "['colour', 'kids', 'winitwednesday']\n",
      "['csm', 'education']\n",
      "['metal', 'metalmaniacs']\n",
      "['adult', 'earnmoney', 'porn']\n",
      "['cleveland', 'hiring', 'hospitality', 'jo']\n",
      "['mtvstars', 'mtvstars', 'mtvstars']\n",
      "['mtvstars', 'mtvstars']\n",
      "['inromowetrust', 'wedemboyz']\n",
      "['cocktails', 'lunch', 'southend', 'yum']\n",
      "['beauty', 'beverlyhills', 'plasticsurgery']\n",
      "['blackfriday', 'sm', 'socialmedia']\n",
      "['interview', 'mvc', 'youtube']\n",
      "['bi', 'bigdata']\n",
      "['confidence', 'mindfulness']\n",
      "['deephouse', 'edm', 'scifi']\n",
      "['tomorrowwhenigetthevampsalbumiwill', 'vampsalbumouttomorrow']\n",
      "['balloon', 'director', 'filmmaking', 'indie', 'shortfilm', 'vimeo']\n",
      "['csrracing', 'raceyourfriends']\n",
      "['anaktimur', 'dinner', 'girls', 'happylife']\n",
      "['romance', 'sexy']\n",
      "['healthcare', 'hiring', 'job', 'sacramento']\n",
      "['f4f', 'followme', 'mgwv', 'rt2gain', 'swissnews2015', 'teamfree', 'thecreator2020']\n",
      "['cleveland', 'engineering', 'hiring', 'job', 'jobs']\n",
      "['keeppounding', 'luuuuke', 'panthers', 'sweater']\n",
      "['ditty', 'thanksgiving']\n",
      "['cleveland', 'traffic']\n",
      "['capitaljbb', 'wannagetcloser']\n",
      "['gas', 'oilandgas', 'thankful', 'thanksgiving']\n",
      "['imthankfulfor', 'themuppets']\n",
      "['bible', 'biblestudy', 'christianity', 'jesus', 'scripture']\n",
      "['genre', 'music']\n",
      "['caramel', 'chocolate', 'pecans', 'recipe']\n",
      "['pics', 'tattoo']\n",
      "['conservative', 'cruz2016']\n",
      "['democrats', 'happythanksgiving']\n",
      "['afghan', 'mohamed_bin_zayed']\n",
      "['berlin', 'graffiti', 'streetart']\n",
      "['bringbackmst3k', 'gorgo', 'mst3kturkeyday']\n",
      "['더쇼', '빅스lr']\n",
      "['behindthescenes', 'sax']\n",
      "['gay', 'gaybooty', 'gaybottom', 'gaybutt', 'gayfeet', 'gayhot', 'gaysexy', 'gaytop']\n",
      "['blackfriday', 'premierleague', 'usafricabf']\n",
      "['mtvstars', 'mtvstars', 'mtvstars']\n",
      "['16days', 'wad2015']\n",
      "['aunifyinggrip', 'gunviolence', 'shameworld']\n",
      "['500px', 'architecture', 'art', 'city']\n",
      "['activism', 'environment']\n"
     ]
    }
   ],
   "source": [
    "# Raw tweets to hashtag pairs\n",
    "if raw_tweets_2_hashtag_sets:\n",
    "    hashtags = findHashtagSets(tweets)\n",
    "    writeList2txt(tweets, hashtag_sets_filename, deliminator = '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Building a network from co-occuring hashtags\n",
    "\n",
    "**Note:** The file hashtag_sets.txt can be interpreted as a hypergraph network as each line (tweet) being a node and the hashtags being the associated groups that they are associated with. Analysis of this network doesn't provide us with as much information about the combination of hashtags being together (i.e. they aren't the nodes of the network as we want)\n",
    "\n",
    "1. Create an empty weighted undirected network.\n",
    "2. Read in each sets of hashtags and create nodes (hashtags) and an edge linking them (or increase the weight of existing edges).\n",
    "3. Use the generated network and save it as an .edgelist file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Detecting communities in the network\n",
    "\n",
    "1. Use the nx_comm.label_propagation_communities to create a list of the community sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Finding the most meaningful communiites\n",
    "Analysis withough preprocessing doesn't give as much detailed information... so a few things should do make it more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods:\n",
    "1. Introducing a weighting threashold on network edges\n",
    "\n",
    "Ignoring the low weighted edges essentially means that a certain number of tweets are required to share hashtags before they are considered connected. Raising the threshold higher will gradually restrict the hashtags that are linked to thoose that are used together very frequently. It would also eliminate the connections for lesser used hashtags as they are less likely to have enough tweets to overcome the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Eliminating smaller connected components of the graph\n",
    "\n",
    "By eliminating connected components you would be able to better focus on the more prevelent hashtags and their relationship with each other. This would also eliminate outlier hashtags that may be mispelled or just not very common. By doing this though (depending on if a weighting threshold was already implimented) very niche hashtags and combinations that could very well be very often used with other hashtags, but not the main ones, would be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting Threshold Implimentation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller Component Elimination\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histagram plot of community sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for when I get here:\n",
    "\n",
    "# plt.hist(comm_sizes,20)\n",
    "# plt.xlabel('community sizes')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of histagram:**\n",
    "\n",
    "notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of certain community examples\n",
    "\n",
    "notes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
